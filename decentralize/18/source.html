<!-- DHTs: Distributed hash tables -->
<h3>Content-addressable data</h3>
<p>The simplest type of database is probably the key-value store: it couples a potentially long string (the value) to a potentially short string (the key), so that you can later retrieve the value that was stored, by presenting its key. Generally, it is possible to store any value under any key. But if for a key-value store, the key of each item is uniquely determined by the value, we say the data is content-addressable: if you know the content (the value), then you know its address (the key).</p>
<p>Content-addressable data stores are efficient when the same value is stored unencrypted many times in the same store: each duplicate entry will map to the same key, meaning the data is automatically de-duplicated. On the other hand, the limitation that you cannot choose the key at the time of storing some data, means that in practice content-addressable data stores are only useful when combine with a generic key-value store.</p>
<p>Storing something in such a "blobs+index" store means that large data items are first stored as blobs in a content-addressable data store, mapping them onto short strings which are deterministically derived from the data item in question. Usually some sort of hashing function is used for this.</p>
<p>And then the second step is to store the hash of the blob in the "index", where you can give it the variable name under which you will want to find it back later.</p>

<h3>A ring of hashes</h3>
<p>When storing content-addressable data redundantly on a cluster of servers, you probably want to be able to add and remove servers without taking the cluster down. If you generate the keys from the values using a hashing function that more or less randomly spreads the resulting keys over their namespace, then you can use that property to easily and fairly determine which servers should store which item.</p>
<p>Imagine the namespace of hashes as a circle, where each point on the circle corresponds to a certain item key. Then you can deploy the servers you have available to each serve an overlapping section of that circle.</p>
<p>You may introduce redundancy by making the sections overlap. Then, if a node fails, you can redistribute the nodes by moving them gradually, so that they elastically adapt. Likewise, if you want to add more servers, you can insert them at a random position on the circle, and gradually let the new set of servers settle to their optimal distribution.</p>
<p>Moving a server clockwise or anti-clockwise on the server mean forgetting items on one end of its circle section, while at the same time obtaining new ones at the other end. If you make sure that each item is always stored at least twice (by overlapping each section 50% with each of its neighbors), then if one node fails, the nodes that neighbored it can immediately start moving towards each other to get full duplicated coverage of the section again.</p>
<p>This doesn't help of course when two adjacent nodes fail, and if you lose a server, then the load on the remaining servers of course becomes proportionally bigger. But running a cluster like this can be a lot easier for a sysadmin then running, say, a master-slave cluster of database servers.</p>

<h3>Erasure coding</h3>
<p>Taking this concept to a more generic level, leads to erasure coding, a strategy implemented by for instance tahoe-lafs. Here, each blob is stored <code>n</code> times instead of necessarily exactly two times. Advantages of storing data multiple times include:</p>
<ul>
<li>disaster recovery, include easy organization of maintenance windows,</li>
<li>easy expansion and reduction of the cluster size in response to throughput and storage size demand,</li>
<li>increased throughput when the traffic is not uniform (for instance, if only one item is being retrieved at a given time, you can use all servers that store it to serve a part of the item's value),</li>
<li>possibility of using commodity, semi-trusted servers, thanks to the inherent robustness of the architecture.</li>
</ul>

<h3>Convergent encryption</h3>
<p>Encrypted blobs can easily be stored as content-addressable data. Thus, you can send a short string which would then translate to a potentially very long string.</p>
<p>To set this up you need a trusted server that does the encryption, and a set of semi-trusted servers that do the actual blob storage.</p>
<p>Do keep in mind that if two people both encrypt the same file (say, a specific webm-file, or a popular linux install CD), then they will not generally result in the same blob, and thus the system will not be able to take advantage of de-duplication.</p>
<p>Convergent encryption fixes this. It uses a different encryption key for each blobs, and derives that encryption key from the blob's entropy, such that it's not derivable from the address key (the blob's hash)</p>
<p>Bitcasa, the company that was the subject of the TechCrunch scandal a couple of years ago, claimed that it could
do both encryption and de-duplication, using this
 <a href="http://techpinions.com/bitcasas-clever-encryption-trick/2677">clever trick</a>.
For Mega.co.nz it is
<a href="http://arstechnica.com/business/2013/01/megabad-a-quick-look-at-the-state-of-megas-encryption/">believed</a>
that they may also be using convergent encryption, although this has not been confirmed by the company itself.</p>
<p>It is important to note, as this last link also explains, that de-duplication exposes some information about the data people are storing.
For instance, a copyright holder could upload their own movie to Mega.co.nz, and ask Mega to report which users own a file that deduplicates against that.</p>
<p>So depending on the reasons you had to encrypt data in the first place, convergent encryption may not be what you are looking for. You may need to use an encryption technique that makes de-duplication impossible.</p>

<h3>Indexes to the data</h3>
<p>As already stated earlier, content-addressable data by itself is in a way useless by definition. Nobody wants to use hashes as variable names. You need an extra layer that translates ultimately human-memorable, or at least human-intelligible labels to the machine-generated hashes that allow looking up the actual full data items in the DHT.</p>
<p>A good example of this is git: it stores blobs, and then trees of commits that references these blobs. This is also how git does de-duplication, which can be quite important depending on how you branch and move files around in your version control repo.</p>

<!--
<h3>Zooko's triangle</h3>
<p>When naming data, you cannot have security, memorability, and decentralization at the same time. You have to choose at most two out of three.</p>
<p>For instance, the DNS system provides names that aer

<h3>Uniqueness through domination</h3>
<p>There is in practice a way out of the dilemma introduced by Zooko's triangle: since our planet has a finite size, and a finite population, it is possible to populate all instances of something with the same data, thus in practice (though not in theory) removing the need to choose one authorative instance.</p>
<p>DNS, TLS, PGP keyservers, and Mainline DHT all use this approach.</p>
-->

<h3>Conclusion</h3>
<p>When storing small items of data, you need an actual key-value store. When the items are big, though, you can combine a first-level key-value store with a second-level content-addressable store, such that the first forms an index to the latter.</p>
<p>Doing this opens up opportunities to use semi-trusted servers on the second level.</p>
<!--<p>Truly decentralized storage of key-value data is impossible, but it can be simulated by dominating the finite set of Earthly instances of a certain service.</p>-->
<p>As always, <a href="https://groups.google.com/forum#!forum/unhosted">comments welcome</a>!</p>
